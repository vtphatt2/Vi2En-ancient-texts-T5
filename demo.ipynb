{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/VoThinhPhat/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from glob import glob \n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "json_files = glob(os.path.join(DATA_FOLDER, '*.json'))\n",
    "json_files.sort()\n",
    "\n",
    "vi_texts = []\n",
    "en_texts = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "        for item in json_data['data']:\n",
    "            vi_texts.append(\"vi: \" + item['vi'].lower())\n",
    "            en_texts.append(\"en: \" + item['en'].lower())\n",
    "\n",
    "data_dict = {\n",
    "    \"vi\": vi_texts,\n",
    "    \"en\": en_texts\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d283b9be2c42c98fcf554fa3059c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"vi\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    targets = tokenizer(examples[\"en\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Extract predictions and labels\n",
    "    predictions, labels = pred\n",
    "    \n",
    "    # Decode the predicted token IDs and the ground truth token IDs\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = train_test_split(dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",  \n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,        \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from evaluate import load\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Load and prepare the dataset\n",
    "with open(\"demo.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Extract the 'vi' and 'en' columns as separate lists with language prefixes\n",
    "vi_texts = [\"vi: \" + item[\"vi\"] for item in dataset[\"data\"]]\n",
    "en_texts = [\"en: \" + item[\"en\"] for item in dataset[\"data\"]]\n",
    "\n",
    "# Create a dictionary where each key is a column name, and the value is a list of values\n",
    "data_dict = {\n",
    "    \"vi\": vi_texts,\n",
    "    \"en\": en_texts\n",
    "}\n",
    "\n",
    "# Create a Hugging Face dataset from the dictionary\n",
    "train_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the \"vi\" column as the input and \"en\" column as the target\n",
    "    inputs = tokenizer(examples[\"vi\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(examples[\"en\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Initialize the data collator for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "bleu_metric = load(\"bleu\")\n",
    "def compute_metrics(pred):\n",
    "    # Extract predictions and labels\n",
    "    predictions, labels = pred\n",
    "    # Decode the predicted token IDs and the ground truth token IDs\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Đánh giá sau mỗi epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,          # Tăng số epoch\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=8  # Tích lũy gradient qua 8 batch\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
